{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Used Car Price Regression\nIn this notebook, Used Price Car will be predicted through ensemble of tuned XGBoost, LGBM, and CatBoost.\n","metadata":{}},{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd              # For data manipulation and analysis\nimport numpy as np               # For numerical computing\nfrom datetime import datetime\nimport scipy.stats as stats      # For statistical analysis\nimport math\nimport matplotlib                # For plotting and visualization\nimport matplotlib.pyplot as plt  \nfrom pandas.plotting import parallel_coordinates\nimport seaborn as sns            # For statistical data visualization\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-16T20:53:18.616344Z","iopub.execute_input":"2024-09-16T20:53:18.616874Z","iopub.status.idle":"2024-09-16T20:53:21.600363Z","shell.execute_reply.started":"2024-09-16T20:53:18.616823Z","shell.execute_reply":"2024-09-16T20:53:21.598816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For machine learning\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import log_evaluation, early_stopping\nfrom catboost import CatBoostRegressor, Pool\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVR\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:21.602934Z","iopub.execute_input":"2024-09-16T20:53:21.603669Z","iopub.status.idle":"2024-09-16T20:53:24.616459Z","shell.execute_reply.started":"2024-09-16T20:53:21.603615Z","shell.execute_reply":"2024-09-16T20:53:24.614910Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load datasets","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv',index_col=0)\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv', index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:24.621681Z","iopub.execute_input":"2024-09-16T20:53:24.622482Z","iopub.status.idle":"2024-09-16T20:53:26.528500Z","shell.execute_reply.started":"2024-09-16T20:53:24.622426Z","shell.execute_reply":"2024-09-16T20:53:26.526977Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:26.531571Z","iopub.execute_input":"2024-09-16T20:53:26.532090Z","iopub.status.idle":"2024-09-16T20:53:26.564663Z","shell.execute_reply.started":"2024-09-16T20:53:26.532043Z","shell.execute_reply":"2024-09-16T20:53:26.563084Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:26.566234Z","iopub.execute_input":"2024-09-16T20:53:26.566684Z","iopub.status.idle":"2024-09-16T20:53:26.588155Z","shell.execute_reply.started":"2024-09-16T20:53:26.566631Z","shell.execute_reply":"2024-09-16T20:53:26.586255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.shape, df_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:26.590074Z","iopub.execute_input":"2024-09-16T20:53:26.590598Z","iopub.status.idle":"2024-09-16T20:53:26.603479Z","shell.execute_reply.started":"2024-09-16T20:53:26.590553Z","shell.execute_reply":"2024-09-16T20:53:26.601903Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"markdown","source":"Distribution of Target - Price","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.histplot(data = df_train['price'], kde = False)\n\n# Set the main title\nplt.suptitle('Distribution of Price', fontsize = 10)\n\n# Set axis labels\nax.set_xlabel('')\nax.set_xticklabels('')\nax.tick_params(left = False, bottom = False)\n\n# Set the data labels\nax.get_xaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p:\"{:.2f}M\".format(x / 1e6)))\n\nsns.despine(right = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:26.605272Z","iopub.execute_input":"2024-09-16T20:53:26.605764Z","iopub.status.idle":"2024-09-16T20:53:32.156788Z","shell.execute_reply.started":"2024-09-16T20:53:26.605710Z","shell.execute_reply":"2024-09-16T20:53:32.155413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Explore the distribution of each numeric variables ","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:32.158466Z","iopub.execute_input":"2024-09-16T20:53:32.159020Z","iopub.status.idle":"2024-09-16T20:53:32.360090Z","shell.execute_reply.started":"2024-09-16T20:53:32.158972Z","shell.execute_reply":"2024-09-16T20:53:32.358175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select numeric columns\nnumeric_columns = df_train.select_dtypes(include=['float64', 'int64'])\n\n# Calculate correlation matrix\ncorrelation_matrix = numeric_columns.corr()\n\n# Visualize correlation with price\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix[['price']].sort_values(by='price', ascending=False), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation of Numeric Features with Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:32.362019Z","iopub.execute_input":"2024-09-16T20:53:32.362512Z","iopub.status.idle":"2024-09-16T20:53:32.760631Z","shell.execute_reply.started":"2024-09-16T20:53:32.362463Z","shell.execute_reply":"2024-09-16T20:53:32.759042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Explore the distribution of each categorical variables ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x='brand', y='price', data=df_train)\n\n# Customize the plot\nplt.title('Price Distribution by Brand')\nplt.xticks(rotation=90)  # Rotate x-axis labels if needed\nplt.xlabel('Brand')\nplt.ylabel('Price')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:32.767805Z","iopub.execute_input":"2024-09-16T20:53:32.768499Z","iopub.status.idle":"2024-09-16T20:53:34.354198Z","shell.execute_reply.started":"2024-09-16T20:53:32.768442Z","shell.execute_reply":"2024-09-16T20:53:34.352785Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x='fuel_type', y='price', data=df_train)\n\n# Customize the plot\nplt.title('Price Distribution by Fuel Type')\nplt.xticks(rotation=90)  # Rotate x-axis labels if needed\nplt.xlabel('Model')\nplt.ylabel('Price')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:34.355911Z","iopub.execute_input":"2024-09-16T20:53:34.356406Z","iopub.status.idle":"2024-09-16T20:53:34.997896Z","shell.execute_reply.started":"2024-09-16T20:53:34.356361Z","shell.execute_reply":"2024-09-16T20:53:34.995729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x='accident', y='price', data=df_train)\n\n# Customize the plot\nplt.title('Price Distribution by Accident')\nplt.xlabel('Accident')\nplt.ylabel('Price')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:34.999613Z","iopub.execute_input":"2024-09-16T20:53:35.000085Z","iopub.status.idle":"2024-09-16T20:53:35.532846Z","shell.execute_reply.started":"2024-09-16T20:53:35.000039Z","shell.execute_reply":"2024-09-16T20:53:35.531197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering\nCredit Feature Engineering to https://www.kaggle.com/code/danishyousuf19/regression-of-used-car-prices","metadata":{}},{"cell_type":"code","source":"def extract_accident_binary(df):\n    \n    mapping = {\n        'None reported': 0,\n        'At least 1 accident or damage reported': 1\n    }\n    df['accident'] = df['accident'].map(mapping)\n    \n    return df\n\n# Apply the function to the dataframe\ndf_train = extract_accident_binary(df_train)\ndf_test = extract_accident_binary(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:35.534722Z","iopub.execute_input":"2024-09-16T20:53:35.535298Z","iopub.status.idle":"2024-09-16T20:53:35.587541Z","shell.execute_reply.started":"2024-09-16T20:53:35.535239Z","shell.execute_reply":"2024-09-16T20:53:35.586093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_data(df):\n    # Ensure the original DataFrame remains intact\n    df = df.copy()\n    df['transmission'] = df['transmission'].str.lower()\n    # Extract horsepower, engine size, cylinders, and transmission speed\n    df['horsepower'] = df['engine'].str.extract(r'(\\d+\\.\\d+)(?=HP)').astype(float)\n    df['engine_size'] = df['engine'].str.extract(r'(\\d+\\.\\d+)(?=L)').astype(float)\n    df['cylinders'] = df['engine'].str.extract(r'(\\d+)\\s(Cylinder|V\\d|Straight)')[0].astype(float)\n#     df['transmission_speed'] = df['transmission'].str.extract(r'(\\d+)(?=-)').astype(float)\n    \n    # Classify transmission type\n    df['transmission_type'] = df['transmission'].apply(lambda x: \n                                                       'manual' if 'm/t' in x or 'manual' in x or  'mt' in x else \n                                                       'automatic' if 'a/t' in x or 'automatic' in x or  'at' in x else \n                                                       'CVT' if 'CVT' in x else \n                                                       'Other')\n    \n    return df\n\n# Apply the function\ndf_train = extract_data(df_train)\ndf_test = extract_data(df_test)  \n\n# Print specific columns to check results\nprint(df_train[['transmission', 'transmission_type']])\n\n# Check the shape to ensure no columns are removed\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:35.589800Z","iopub.execute_input":"2024-09-16T20:53:35.590460Z","iopub.status.idle":"2024-09-16T20:53:40.829528Z","shell.execute_reply.started":"2024-09-16T20:53:35.590398Z","shell.execute_reply":"2024-09-16T20:53:40.827482Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to extract fuel type from the engine column\ndef extract_fuel_type(engine_info):\n    if pd.isna(engine_info):\n        return np.nan\n    if 'Gasoline' in engine_info:\n        return 'Gasoline'\n    elif 'Hybrid' in engine_info:\n        return 'Hybrid'\n    elif 'Flex Fuel' in engine_info or 'E85' in engine_info:\n        return 'Flex Fuel'\n    elif 'Diesel' in engine_info:\n        return 'Diesel'\n    elif 'Electric' in engine_info:\n        return 'Electric'\n    else:\n        return np.nan\n\ndf_train['extracted_fuel_type'] = df_train['engine'].apply(extract_fuel_type)\n\ndf_train['fuel_type'].fillna(df_train['extracted_fuel_type'], inplace=True)\n\ndf_train.drop(columns=['extracted_fuel_type'], inplace=True)\ndf_test['extracted_fuel_type'] = df_test['engine'].apply(extract_fuel_type)\n\ndf_test['fuel_type'].fillna(df_test['extracted_fuel_type'], inplace=True)\n\ndf_test.drop(columns=['extracted_fuel_type'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:40.831846Z","iopub.execute_input":"2024-09-16T20:53:40.832512Z","iopub.status.idle":"2024-09-16T20:53:41.513245Z","shell.execute_reply.started":"2024-09-16T20:53:40.832447Z","shell.execute_reply":"2024-09-16T20:53:41.511471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fill_clean_title(row):\n    if pd.isna(row['clean_title']):\n        if row['accident'] == 0:\n            return True\n        elif row['accident'] == 1:\n            return False\n    return row['clean_title']\n\n# Apply the function to each row\ndf_train['clean_title'] = df_train.apply(fill_clean_title, axis=1)\ndf_test['clean_title'] = df_test.apply(fill_clean_title, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:41.515008Z","iopub.execute_input":"2024-09-16T20:53:41.515572Z","iopub.status.idle":"2024-09-16T20:53:48.258793Z","shell.execute_reply.started":"2024-09-16T20:53:41.515501Z","shell.execute_reply":"2024-09-16T20:53:48.257089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature(df):\n    current_year = datetime.now().year\n    df['Vehicle_Age'] = current_year - df['model_year']\n    df['Mileage_per_Year'] = df['milage'] / (df['Vehicle_Age']+10e-5)\n    luxury_brands =   [\n    \"Mercedes-Benz\", \"BMW\", \"Audi\", \"Porsche\", \"Land Rover\", \n    \"Lexus\", \"Cadillac\", \"Tesla\", \"INFINITI\", \"Jaguar\", \n    \"Bentley\", \"Maserati\", \"Lamborghini\", \"Genesis\", \"Rolls-Royce\", \n    \"Ferrari\", \"McLaren\", \"Aston Martin\", \"Lucid\", \"Lotus\", \n    \"Karma\", \"Bugatti\", \"Maybach\"    ]\n    df['Is_Luxury_Brand'] = df['brand'].apply(lambda x: 1 if x in luxury_brands else 0)\n    return df\n\ndf_train = feature(df_train)\ndf_test = feature(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:48.260902Z","iopub.execute_input":"2024-09-16T20:53:48.261838Z","iopub.status.idle":"2024-09-16T20:53:48.604221Z","shell.execute_reply.started":"2024-09-16T20:53:48.261770Z","shell.execute_reply":"2024-09-16T20:53:48.602041Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_base_colors(df, column_name):\n    # Define a list of common base colors\n    base_colors = [\n        'black', 'white', 'gray', 'silver', 'brown', 'red', 'blue', 'green',\n        'beige', 'tan', 'orange', 'gold', 'yellow', 'purple', 'pink', \n        'charcoal', 'ivory', 'camel', 'chestnut', 'pearl', 'linen', 'graphite',\n        'copper', 'slate', 'bronze', 'sand', 'amber','macchiato','ebony','cocoa'\n    ]\n    df[column_name] = df[column_name].str.lower()\n\n    def find_base_color(text):\n        for color in base_colors:\n            if color in text:\n                return color\n        return text  \n    \n    df['int_col'] = df[column_name].apply(find_base_color)\n    df['ext_col'] = df[column_name].apply(find_base_color)\n\n    return df\ndf_train=extract_base_colors(df_train,'int_col')\ndf_test=extract_base_colors(df_test,'int_col')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:48.606734Z","iopub.execute_input":"2024-09-16T20:53:48.607319Z","iopub.status.idle":"2024-09-16T20:53:49.106609Z","shell.execute_reply.started":"2024-09-16T20:53:48.607251Z","shell.execute_reply":"2024-09-16T20:53:49.104829Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train=df_train.drop(columns=['transmission','engine','transmission_type','model'],axis=1)\ndf_test=df_test.drop(columns=['transmission','engine','transmission_type','model'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:49.108599Z","iopub.execute_input":"2024-09-16T20:53:49.109068Z","iopub.status.idle":"2024-09-16T20:53:49.186635Z","shell.execute_reply.started":"2024-09-16T20:53:49.109018Z","shell.execute_reply":"2024-09-16T20:53:49.184110Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_feats = ['brand',  'fuel_type',  'ext_col', 'int_col', 'clean_title']  \nnumeric_feats = ['milage', 'horsepower', 'engine_size', 'cylinders','accident','Is_Luxury_Brand','Mileage_per_Year','Vehicle_Age','cylinders']","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:49.189914Z","iopub.execute_input":"2024-09-16T20:53:49.190596Z","iopub.status.idle":"2024-09-16T20:53:49.200195Z","shell.execute_reply.started":"2024-09-16T20:53:49.190524Z","shell.execute_reply":"2024-09-16T20:53:49.198326Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cleaning(df, cat_feats, threshold=101):\n    for i in cat_feats:\n        if df[i].dtype.name == 'category':\n            if 'missing' not in df[i].cat.categories:\n                df[i] = df[i].cat.add_categories('missing')\n            if 'noise' not in df[i].cat.categories:\n                df[i] = df[i].cat.add_categories('noise')\n        else:\n            df[i] = df[i].astype('category')\n            df[i] = df[i].cat.add_categories(['missing', 'noise'])\n        \n        df[i] = df[i].fillna('missing')\n        \n        count = df[i].value_counts(dropna=False)\n        less_freq = count[count < threshold].index\n        \n        df[i] = df[i].apply(lambda x: 'noise' if x in less_freq else x)\n    \n    return df\ndf_train = cleaning(df_train, cat_feats)\ndf_test = cleaning(df_test, cat_feats) ","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:49.202173Z","iopub.execute_input":"2024-09-16T20:53:49.202794Z","iopub.status.idle":"2024-09-16T20:53:49.558120Z","shell.execute_reply.started":"2024-09-16T20:53:49.202727Z","shell.execute_reply":"2024-09-16T20:53:49.556073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_cleaning_binary(df):\n    \n    mapping = {\n        'Yes': 1,\n        'missing': 0,\n        'noise': 0,\n        True:1,\n        False:0\n    }\n    df['clean_title'] = df['clean_title'].map(mapping)\n    return df\n\n# Apply the function to the dataframe\ndf_train = extract_cleaning_binary(df_train)\ndf_test = extract_cleaning_binary(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:49.560709Z","iopub.execute_input":"2024-09-16T20:53:49.561201Z","iopub.status.idle":"2024-09-16T20:53:49.576443Z","shell.execute_reply.started":"2024-09-16T20:53:49.561155Z","shell.execute_reply":"2024-09-16T20:53:49.574656Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in cat_feats:\n    df_train[i] = df_train[i].astype('category')\n    df_test[i] = df_test[i].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:49.578449Z","iopub.execute_input":"2024-09-16T20:53:49.578894Z","iopub.status.idle":"2024-09-16T20:53:49.734242Z","shell.execute_reply.started":"2024-09-16T20:53:49.578849Z","shell.execute_reply":"2024-09-16T20:53:49.732948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert columns to numeric\ndf_train['horsepower'] = pd.to_numeric(df_train['horsepower'], errors='coerce')\ndf_train['engine_size'] = pd.to_numeric(df_train['engine_size'], errors='coerce')\ndf_train['cylinders'] = pd.to_numeric(df_train['cylinders'], errors='coerce')\ndf_train['Vehicle_Age'] = pd.to_numeric(df_train['Vehicle_Age'], errors='coerce')\ndf_train['Mileage_per_Year'] = pd.to_numeric(df_train['Mileage_per_Year'], errors='coerce')\ndf_train['price'] = pd.to_numeric(df_train['price'], errors='coerce')\n\n# Convert categorical columns to numeric using Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf_train['brand'] = le.fit_transform(df_train['brand'])\ndf_train['fuel_type'] = le.fit_transform(df_train['fuel_type'])\ndf_train['ext_col'] = le.fit_transform(df_train['ext_col'])\ndf_train['int_col'] = le.fit_transform(df_train['int_col'])\ndf_train['clean_title'] = le.fit_transform(df_train['clean_title'])\n\n# Handle missing values if any\ndf_train.fillna(df_train.mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:49.735768Z","iopub.execute_input":"2024-09-16T20:53:49.736221Z","iopub.status.idle":"2024-09-16T20:53:50.059311Z","shell.execute_reply.started":"2024-09-16T20:53:49.736168Z","shell.execute_reply":"2024-09-16T20:53:50.057553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert columns to numeric\ndf_test['horsepower'] = pd.to_numeric(df_test['horsepower'], errors='coerce')\ndf_test['engine_size'] = pd.to_numeric(df_test['engine_size'], errors='coerce')\ndf_test['cylinders'] = pd.to_numeric(df_test['cylinders'], errors='coerce')\ndf_test['Vehicle_Age'] = pd.to_numeric(df_test['Vehicle_Age'], errors='coerce')\ndf_test['Mileage_per_Year'] = pd.to_numeric(df_test['Mileage_per_Year'], errors='coerce')\n\n# Convert categorical columns to numeric using Label Encoding\nle = LabelEncoder()\ndf_test['brand'] = le.fit_transform(df_test['brand'])\ndf_test['fuel_type'] = le.fit_transform(df_test['fuel_type'])\ndf_test['ext_col'] = le.fit_transform(df_test['ext_col'])\ndf_test['int_col'] = le.fit_transform(df_test['int_col'])\ndf_test['clean_title'] = le.fit_transform(df_test['clean_title'])\n\n# Handle missing values if any\ndf_test.fillna(df_test.mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:50.062008Z","iopub.execute_input":"2024-09-16T20:53:50.062881Z","iopub.status.idle":"2024-09-16T20:53:50.285032Z","shell.execute_reply.started":"2024-09-16T20:53:50.062785Z","shell.execute_reply":"2024-09-16T20:53:50.282901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:50.287215Z","iopub.execute_input":"2024-09-16T20:53:50.287887Z","iopub.status.idle":"2024-09-16T20:53:50.319969Z","shell.execute_reply.started":"2024-09-16T20:53:50.287833Z","shell.execute_reply":"2024-09-16T20:53:50.317580Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(['price'], axis=1)\ny = df_train['price']\n\nX.shape,y.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:50.322495Z","iopub.execute_input":"2024-09-16T20:53:50.323306Z","iopub.status.idle":"2024-09-16T20:53:50.347916Z","shell.execute_reply.started":"2024-09-16T20:53:50.323242Z","shell.execute_reply":"2024-09-16T20:53:50.346116Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.02, random_state=12)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:50.355752Z","iopub.execute_input":"2024-09-16T20:53:50.356287Z","iopub.status.idle":"2024-09-16T20:53:50.452374Z","shell.execute_reply.started":"2024-09-16T20:53:50.356241Z","shell.execute_reply":"2024-09-16T20:53:50.450716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"def cross_validate_model_x(model, X_train, y_train, X_valid, y_valid, params, n_splits=5):\n\n    # Initialize variables\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    val_scores = []\n    test_preds = np.zeros((len(df_test), n_splits), dtype=np.float32)\n    # Cross-validation loop\n    for fold, (train_ind, valid_ind) in enumerate(cv.split(X_train)):\n        # Data splitting\n        X_fold_train = X.iloc[train_ind]\n        y_fold_train = y.iloc[train_ind]\n        X_val = X.iloc[valid_ind]\n        y_val = y.iloc[valid_ind]\n        \n        # Model initialization and training\n        clf = model(**params, enable_categorical=True)\n        clf.fit(X_fold_train, y_fold_train,  eval_set=[(X_val, y_val)],verbose=500, early_stopping_rounds=50)\n        # Predict and evaluate\n        test_preds[:, fold] = clf.predict(df_test)\n\n        print(\"-\" * 50)\n        print(test_preds)\n    \n    # Train on full X_train after cross-validation\n    clf.fit(X_train, y_train)\n    \n    # Predict on X_valid\n    valid_preds = clf.predict(X_valid)\n    \n    # Evaluate performance on X_valid\n    valid_score = mean_squared_error(y_valid, valid_preds, squared=False)  # RMSE evaluation\n    print(f\"Validation RMSE: {valid_score}\")\n    \n    # Average test predictions from cross-validation\n    test_preds = np.mean(test_preds, axis=1)\n    \n    return clf, test_preds, valid_score","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:53:50.454930Z","iopub.execute_input":"2024-09-16T20:53:50.455468Z","iopub.status.idle":"2024-09-16T20:53:50.472389Z","shell.execute_reply.started":"2024-09-16T20:53:50.455418Z","shell.execute_reply":"2024-09-16T20:53:50.470518Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the objective function for Optuna\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 10000),\n        'max_depth': trial.suggest_int('max_depth', 10, 50),\n        'min_child_weight': trial.suggest_int('min_child_weight', 50, 100),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n        'booster': 'gbtree',\n        'objective':'reg:squarederror',\n        'eval_metric':'rmse',\n        'random_state': 42\n    }\n    \n    rmse_scores = []\n\n    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n    # Train XGBoost model with current hyperparameters\n    clf = XGBRegressor(**params, use_label_encoder=False, enable_categorical=True,tree_method='hist',device= 'cuda')\n    clf.fit(X_train_split, y_train_split)\n\n    # Predict probabilities on validation set\n    y_pred = clf.predict(X_valid_split)\n\n    # Calculate RMSE on validation set\n    rmse = mean_squared_error(y_valid_split, y_pred, squared=False)\n    return rmse\n\n# Optimize hyperparameters using Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n# Get best hyperparameters\nxgb_best_params = study.best_params\nprint(\"Best Hyperparameters:\", xgb_best_params)","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-09-16T20:53:50.474265Z","iopub.execute_input":"2024-09-16T20:53:50.475778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params={'booster': 'gbtree',\n             'objective':'reg:squarederror',\n             'eval_metric':'rmse',\n             'random_state': 42,\n             'lambda': 0.03880258557285165,\n             'alpha': 0.02129832295514386,\n             'colsample_bytree': 0.4,\n             'subsample': 0.7,\n             'learning_rate': 0.014,\n             'max_depth': 17,\n             'min_child_weight': 85,\n             'n_estimators': 10000\n} \nprint('XGBoost Cross-Validation Results:\\n')\nxgb_model, test_predsx, xgb_val_score = cross_validate_model_x(XGBRegressor, X_train, y_train, X_valid, y_valid, xgb_best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_result =  pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\nxgb_result['price'] = test_predsx.astype(np.float32)\nxgb_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"def cross_validate_model_l(model, X_train, y_train, X_valid, y_valid, params, n_splits=5):\n\n    # Initialize variables\n    callbacks = [log_evaluation(period=150), early_stopping(stopping_rounds=200)]\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    val_scores = []\n    test_preds = np.zeros((len(df_test), n_splits), dtype=np.float32)\n    # Cross-validation loop\n    for fold, (train_ind, valid_ind) in enumerate(cv.split(X_train)):\n        # Data splitting\n        X_fold_train = X_train.iloc[train_ind]\n        y_fold_train = y_train.iloc[train_ind]\n        X_val = X_train.iloc[valid_ind]\n        y_val = y_train.iloc[valid_ind]\n        \n        # Model initialization and training\n        clf = model(**params)\n        #clf.fit(X_fold_train, y_fold_train)\n        clf.fit(X_fold_train, y_fold_train, eval_set=[(X_val, y_val)], callbacks=callbacks )\n        # Predict and evaluate\n        test_preds[:, fold] = clf.predict(df_test)\n\n        print(\"-\" * 50)\n        print(test_preds)\n        \n    # Predict on X_valid\n    valid_preds = clf.predict(X_valid)\n    \n    # Evaluate performance on X_valid\n    valid_score = mean_squared_error(y_valid, valid_preds, squared=False)  # RMSE evaluation\n    print(f\"Validation RMSE: {valid_score}\")\n    \n    # Average test predictions from cross-validation\n    test_preds = np.mean(test_preds, axis=1)\n    \n    return clf, test_preds, valid_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the objective function for Optuna\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n        'num_leaves': trial.suggest_int('num_leaves', 32, 200),\n        'max_depth': trial.suggest_int('max_depth', 10, 50),\n        'min_child_weight': trial.suggest_int('min_child_weight', 50, 100),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),  \n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n        'verbose':-1\n    }\n\n    # Train-test split\n    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n    # Initialize LGBM model\n    clf = LGBMRegressor(**params, objective='regression', random_state=42)\n\n    # Fit the model with early stopping and evaluation\n    clf.fit(X_train_split, y_train_split, eval_set=[(X_valid_split, y_valid_split)], eval_metric='rmse')\n\n    # Predict on the validation set\n    y_pred = clf.predict(X_valid_split)\n\n    # Calculate RMSE\n    rmse = mean_squared_error(y_valid_split, y_pred, squared=False)\n    return rmse\n\n# Optimize hyperparameters using Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n# Get best hyperparameters\nlgb_best_params = study.best_params\nprint(\"Best Hyperparameters:\", lgb_best_params)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_params = {\n    'learning_rate': 0.017521301504983752,\n    'max_depth': 42,\n    'reg_alpha': 0.06876635751774487, \n    'reg_lambda': 9.738899198284985,\n    'num_leaves': 131,\n    'subsample': 0.2683765421728044,\n    'colsample_bytree': 0.44346036599709887,\n    'n_estimators': 1000,\n    'random_state': 42          \n}\n\nprint('LightGBM Cross-Validation Results:\\n')\nlgb_model,test_predsl, lgb_val_score = cross_validate_model_l(LGBMRegressor, X_train, y_train, X_valid, y_valid, lgb_best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_result =  pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\nlgb_result['price'] = test_predsl.astype(np.float32)\nlgb_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"def cross_validate_model_c(model, X_train, y_train, X_valid, y_valid, params, n_splits=5):\n\n    # Initialize variables\n    \n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n    val_scores = []\n    test_preds = np.zeros((len(df_test), n_splits), dtype=np.float32)\n    # Cross-validation loop\n    for fold, (train_ind, valid_ind) in enumerate(cv.split(X_train)):\n        # Data splitting\n        X_fold_train = X_train.iloc[train_ind]\n        y_fold_train = y_train.iloc[train_ind]\n        X_val = X_train.iloc[valid_ind]\n        y_val = y_train.iloc[valid_ind]\n        \n        # Model initialization and training\n        clf = model(**params)\n        #clf.fit(X_fold_train, y_fold_train)\n        clf.fit(X_fold_train, y_fold_train, eval_set=[(X_val, y_val)], verbose=0)\n        # Predict and evaluate\n        test_preds[:, fold] = clf.predict(df_test)\n\n        print(\"-\" * 50)\n        print(test_preds)\n    \n    # Predict on X_valid\n    valid_preds = clf.predict(X_valid)\n    \n    # Evaluate performance on X_valid\n    valid_score = mean_squared_error(y_valid, valid_preds, squared=False)  # RMSE evaluation\n    print(f\"Validation RMSE: {valid_score}\")\n    \n    # Average test predictions from cross-validation\n    test_preds = np.mean(test_preds, axis=1)\n    \n    return clf, test_preds, valid_score","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostRegressor\ncat_cols = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Define the objective function for Optuna\ndef objective(trial):\n    params = {\n        'iterations': trial.suggest_int('iterations', 200, 1000),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-8, 10.0),\n        'random_strength': trial.suggest_loguniform('random_strength', 1e-8, 10.0),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n        'random_seed': 42,\n        'early_stopping_rounds':50,\n        'cat_features': cat_cols\n    }\n\n    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n    # Train CatBoost model with current hyperparameters\n    clf = CatBoostRegressor(**params, silent=True)\n    clf.fit(X_train_split, y_train_split, eval_set=[(X_valid_split, y_valid_split)])\n\n    # Predict probabilities on validation set\n    y_pred = clf.predict(X_valid_split)\n\n    # Calculate RMSE on validation set\n    rmse = mean_squared_error(y_valid_split, y_pred, squared=False)\n    return rmse\n\n# Optimize hyperparameters using Optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\n\n# Get best hyperparameters\ncat_best_params = study.best_params\nprint(\"Best Hyperparameters:\", cat_best_params)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_params = {\n    'learning_rate':0.042,\n    'iterations':1000,\n    'depth':10,\n    'random_strength':0,\n    'cat_features':cat_cols,\n    'l2_leaf_reg':0.3,\n    'random_seed':42,\n    'early_stopping_rounds': 200,                     \n}\n\nprint('CatBoost Cross-Validation Results:\\n')\ncat_model, test_predsc, cat_val_score = cross_validate_model_c(CatBoostRegressor, X_train, y_train, X_valid, y_valid, cat_best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_result =  pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\ncat_result['price'] = test_predsc.astype(np.float32)\ncat_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"print('XGBoost Validation RMSE',xgb_val_score)\nprint('LGBM Validation RMSE',lgb_val_score)\nprint('CatBoost Validation RMSE',cat_val_score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"pred = pd.concat([lgb_result,cat_result,xgb_result], axis=1)['price']\nprint(pred)\ntest_preds = test_predsl * 0.6 + test_predsc * 0.3 + test_predsx*0.1","metadata":{"execution":{"iopub.status.busy":"2024-09-15T01:49:17.538225Z","iopub.status.idle":"2024-09-15T01:49:17.538764Z","shell.execute_reply.started":"2024-09-15T01:49:17.538543Z","shell.execute_reply":"2024-09-15T01:49:17.538570Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\ndf_sub['price'] = test_preds\ndf_sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T01:49:17.540394Z","iopub.status.idle":"2024-09-15T01:49:17.540995Z","shell.execute_reply.started":"2024-09-15T01:49:17.540672Z","shell.execute_reply":"2024-09-15T01:49:17.540702Z"},"trusted":true},"outputs":[],"execution_count":null}]}